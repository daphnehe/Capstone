{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289c401b-3a2a-41f7-8a06-636650fb51de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/billpark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/billpark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/billpark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84cfe70-2097-49df-bd6d-a40fe8166354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a07ca4-e038-4a69-a85b-35dfdf92a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSON data into a DataFrame\n",
    "def load_json_to_df(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# List of JSON files\n",
    "json_files = ['HR.json', 'HF.json', 'MR.json', 'MF.json']\n",
    "dfs = []\n",
    "\n",
    "# Load each JSON file into a DataFrame and store them in a list\n",
    "for json_file in json_files:\n",
    "    df = load_json_to_df('data/' + json_file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Naming the DataFrames\n",
    "HR_df, HF_df, MR_df, MF_df = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a67e4f2-4053-4e8a-b4c7-f2236fd53b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f85dd5-974a-4a3a-8813-d6e24a561fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR = HR_df['text']\n",
    "HF = HF_df['text']\n",
    "MR = MR_df['text']\n",
    "MF = MF_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e715af34-8c46-466f-befd-41a6f2665f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([HR, HF, MR, MF], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77fa235b-02cf-4e43-967d-76655ab1a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assigning labels for each comparison\n",
    "labels_hr_vs_mf = [0] * len(HR) + [1] * len(MF)\n",
    "labels_hf_vs_mf = [0] * len(HF) + [1] * len(MF)\n",
    "labels_mr_vs_mf = [0] * len(MR) + [1] * len(MF)\n",
    "labels_hf_vs_mr = [0] * len(HF) + [1] * len(MR)\n",
    "labels_hr_vs_mr = [0] * len(HR) + [1] * len(MR)\n",
    "labels_hr_vs_hf = [0] * len(HR) + [1] * len(HF)\n",
    "\n",
    "#Separate for Human vs AI in general\n",
    "labels_hr_hf = [0] * (len(HR) + len(HF))\n",
    "labels_mr_mf = [1] * (len(MR) + len(MF))\n",
    "\n",
    "labels_human_machine = labels_hr_hf + labels_mr_mf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acdbbcde-dd38-4114-83d1-17b169b1f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for each comparison\n",
    "X_train_hr_vs_mf, X_test_hr_vs_mf, y_train_hr_vs_mf, y_test_hr_vs_mf = train_test_split(pd.concat([HR, MF], ignore_index=True), labels_hr_vs_mf, test_size=0.2, random_state=42)\n",
    "X_train_hf_vs_mf, X_test_hf_vs_mf, y_train_hf_vs_mf, y_test_hf_vs_mf = train_test_split(pd.concat([HF, MF], ignore_index=True), labels_hf_vs_mf, test_size=0.2, random_state=42)\n",
    "X_train_mr_vs_mf, X_test_mr_vs_mf, y_train_mr_vs_mf, y_test_mr_vs_mf = train_test_split(pd.concat([MR, MF], ignore_index=True), labels_mr_vs_mf, test_size=0.2, random_state=42)\n",
    "X_train_hf_vs_mr, X_test_hf_vs_mr, y_train_hf_vs_mr, y_test_hf_vs_mr = train_test_split(pd.concat([HF, MR], ignore_index=True), labels_hf_vs_mr, test_size=0.2, random_state=42)\n",
    "X_train_hr_vs_mr, X_test_hr_vs_mr, y_train_hr_vs_mr, y_test_hr_vs_mr = train_test_split(pd.concat([HR, MR], ignore_index=True), labels_hr_vs_mr, test_size=0.2, random_state=42)\n",
    "X_train_hr_vs_hf, X_test_hr_vs_hf, y_train_hr_vs_hf, y_test_hr_vs_hf = train_test_split(pd.concat([HR, HF], ignore_index=True), labels_hr_vs_hf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the data for the comparison between human-generated and machine-generated text\n",
    "X_train_human_machine, X_test_human_machine, y_train_human_machine, y_test_human_machine = train_test_split(pd.concat([pd.concat([HR, HF]), pd.concat([MR, MF])], ignore_index=True),labels_human_machine,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dacb089-dbf7-4d25-8e91-ab64bfee6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating TF-IDF vectorizer for each comparison\n",
    "tfidf_vectorizer_hr_vs_mf = TfidfVectorizer(max_features=5000)\n",
    "X_train_hr_vs_mf_tfidf = tfidf_vectorizer_hr_vs_mf.fit_transform(X_train_hr_vs_mf)\n",
    "X_test_hr_vs_mf_tfidf = tfidf_vectorizer_hr_vs_mf.transform(X_test_hr_vs_mf)\n",
    "\n",
    "tfidf_vectorizer_hf_vs_mf = TfidfVectorizer(max_features=5000)\n",
    "X_train_hf_vs_mf_tfidf = tfidf_vectorizer_hf_vs_mf.fit_transform(X_train_hf_vs_mf)\n",
    "X_test_hf_vs_mf_tfidf = tfidf_vectorizer_hf_vs_mf.transform(X_test_hf_vs_mf)\n",
    "\n",
    "tfidf_vectorizer_mr_vs_mf = TfidfVectorizer(max_features=5000)\n",
    "X_train_mr_vs_mf_tfidf = tfidf_vectorizer_mr_vs_mf.fit_transform(X_train_mr_vs_mf)\n",
    "X_test_mr_vs_mf_tfidf = tfidf_vectorizer_mr_vs_mf.transform(X_test_mr_vs_mf)\n",
    "\n",
    "tfidf_vectorizer_hf_vs_mr = TfidfVectorizer(max_features=5000)\n",
    "X_train_hf_vs_mr_tfidf = tfidf_vectorizer_hf_vs_mr.fit_transform(X_train_hf_vs_mr)\n",
    "X_test_hf_vs_mr_tfidf = tfidf_vectorizer_hf_vs_mr.transform(X_test_hf_vs_mr)\n",
    "\n",
    "tfidf_vectorizer_hr_vs_mr = TfidfVectorizer(max_features=5000)\n",
    "X_train_hr_vs_mr_tfidf = tfidf_vectorizer_hr_vs_mr.fit_transform(X_train_hr_vs_mr)\n",
    "X_test_hr_vs_mr_tfidf = tfidf_vectorizer_hr_vs_mr.transform(X_test_hr_vs_mr)\n",
    "\n",
    "tfidf_vectorizer_hr_vs_hf = TfidfVectorizer(max_features=5000)\n",
    "X_train_hr_vs_hf_tfidf = tfidf_vectorizer_hr_vs_hf.fit_transform(X_train_hr_vs_hf)\n",
    "X_test_hr_vs_hf_tfidf = tfidf_vectorizer_hr_vs_hf.transform(X_test_hr_vs_hf)\n",
    "\n",
    "# human and machine in general\n",
    "\n",
    "# Create TF-IDF vectorizer for human vs. machine comparison\n",
    "tfidf_vectorizer_human_machine = TfidfVectorizer()\n",
    "X_train_human_machine_tfidf = tfidf_vectorizer_human_machine.fit_transform(X_train_human_machine)\n",
    "X_test_human_machine_tfidf = tfidf_vectorizer_human_machine.transform(X_test_human_machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef4842a1-5373-41fa-9d35-b872824f984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR vs MF:\n",
      "Accuracy: 0.9726941747572816\n",
      "HF vs MF:\n",
      "Accuracy: 0.9675642594859241\n",
      "MR vs MF:\n",
      "Accuracy: 0.9485160508782556\n",
      "HF vs MR:\n",
      "Accuracy: 0.9133858267716536\n",
      "HR vs MR:\n",
      "Accuracy: 0.8672672672672672\n",
      "HR vs HF:\n",
      "Accuracy: 0.779126213592233\n",
      "Human vs Machine:\n",
      "Accuracy: 0.9238932686476653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Function to train and evaluate an XGBoost model\n",
    "def train_and_evaluate_xgb(X_train, y_train, X_test, y_test):\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    return model, predictions \n",
    "\n",
    "# Using the function to train and evaluate models for each comparison\n",
    "def main():\n",
    "    print(\"HR vs MF:\")\n",
    "    train_and_evaluate_xgb(X_train_hr_vs_mf_tfidf, y_train_hr_vs_mf, X_test_hr_vs_mf_tfidf, y_test_hr_vs_mf)\n",
    "    print(\"HF vs MF:\")\n",
    "    train_and_evaluate_xgb(X_train_hf_vs_mf_tfidf, y_train_hf_vs_mf, X_test_hf_vs_mf_tfidf, y_test_hf_vs_mf)\n",
    "    print(\"MR vs MF:\")\n",
    "    train_and_evaluate_xgb(X_train_mr_vs_mf_tfidf, y_train_mr_vs_mf, X_test_mr_vs_mf_tfidf, y_test_mr_vs_mf)\n",
    "    print(\"HF vs MR:\")\n",
    "    train_and_evaluate_xgb(X_train_hf_vs_mr_tfidf, y_train_hf_vs_mr, X_test_hf_vs_mr_tfidf, y_test_hf_vs_mr)\n",
    "    print(\"HR vs MR:\")\n",
    "    train_and_evaluate_xgb(X_train_hr_vs_mr_tfidf, y_train_hr_vs_mr, X_test_hr_vs_mr_tfidf, y_test_hr_vs_mr)\n",
    "    print(\"HR vs HF:\")\n",
    "    train_and_evaluate_xgb(X_train_hr_vs_hf_tfidf, y_train_hr_vs_hf, X_test_hr_vs_hf_tfidf, y_test_hr_vs_hf)\n",
    "    print(\"Human vs Machine:\")\n",
    "    train_and_evaluate_xgb(X_train_human_machine_tfidf, y_train_human_machine, X_test_human_machine_tfidf, y_test_human_machine)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d2a75e-e571-444c-aaee-dd9f349ff3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def display_results(X_test_tfidf, y_test, predictions, vectorizer):\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    \n",
    "    # Convert TF-IDF back to text\n",
    "    X_test_texts = vectorizer.inverse_transform(X_test_tfidf)\n",
    "    \n",
    "    # Finding misclassified and correctly classified indices\n",
    "    misclassified_indices = []\n",
    "    correctly_classified_indices = []\n",
    "    for i, (actual, predicted) in enumerate(zip(y_test, predictions)):\n",
    "        if actual != predicted:\n",
    "            misclassified_indices.append(i)\n",
    "        else:\n",
    "            correctly_classified_indices.append(i)\n",
    "    \n",
    "    # Display examples\n",
    "    print(\"\\nCorrectly Classified Examples:\")\n",
    "    for i in correctly_classified_indices[:3]:  # display first 3 correctly classified examples\n",
    "        print(f\"Text: {' '.join(X_test_texts[i])}\\nLabel: {y_test[i]}\\n\")\n",
    "    \n",
    "    print(\"Misclassified Examples:\")\n",
    "    for i in misclassified_indices[:3]:  # display first 3 misclassified examples\n",
    "        print(f\"Text: {' '.join(X_test_texts[i])}\\nActual Label: {y_test[i]}, Predicted Label: {predictions[i]}\\n\")\n",
    "\n",
    "# # Example usage for one comparison:\n",
    "# print(\"HR vs MF Examples:\")\n",
    "# model_hr_vs_mf, predictions_hr_vs_mf = train_and_evaluate_xgb(X_train_hr_vs_mf_tfidf, y_train_hr_vs_mf, X_test_hr_vs_mf_tfidf, y_test_hr_vs_mf)\n",
    "# display_results(X_test_hr_vs_mf_tfidf, y_test_hr_vs_mf, predictions_hr_vs_mf, tfidf_vectorizer_hr_vs_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae89c07-33be-482c-a624-6c34e4aefb62",
   "metadata": {},
   "source": [
    "### Visuals attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf891e8e-b4fe-463c-94ff-0f97bc737d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_xgb_history(results, comparison_name):\n",
    "    epochs = range(len(results['validation_0']['logloss']))  # Assuming logloss and error are recorded\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, results['validation_0']['logloss'], label='Training Log Loss')\n",
    "    plt.plot(epochs, results['validation_1']['logloss'], label='Validation Log Loss')\n",
    "    plt.title('Log Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, results['validation_0']['error'], label='Training Error')\n",
    "    plt.plot(epochs, results['validation_1']['error'], label='Validation Error')\n",
    "    plt.title('Classification Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(f'Model Training and Validation Metrics for {comparison_name}')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fac6d9e-9c87-473d-af72-0a2895de564d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_hr_vs_mf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_xgb_history(results_hr_vs_mf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR vs MF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_hr_vs_mf' is not defined"
     ]
    }
   ],
   "source": [
    "plot_xgb_history(results_hr_vs_mf, \"HR vs MF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8087e66-97e2-4a1f-aaf8-7fb0c70b68bd",
   "metadata": {},
   "source": [
    "### Word2Vec attemp from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dffec04-0d2a-44f3-bbb5-ff6780e64d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7bc36-a405-44fe-9b95-1a91c9cc94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def build_word2vec_model(data_frames, text_extractor):\n",
    "    sentences = []\n",
    "    for df in data_frames:\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "            for _, row in df.iterrows():\n",
    "                extracted_text = text_extractor(row)\n",
    "                tokenized_text = text_to_wordlist(extracted_text)\n",
    "                sentences.append(tokenized_text)\n",
    "        else:\n",
    "            print(\"Error: Provided data is not a DataFrame or is empty.\")\n",
    "\n",
    "    if sentences:\n",
    "        model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(\"No sentences were extracted for Word2Vec training. Check your data and text extraction logic.\")\n",
    "\n",
    "def vectorize_with_word2vec(df, word2vec_model, text_extractor):\n",
    "    def average_vector(row):\n",
    "        text = text_extractor(row)\n",
    "        words = text_to_wordlist(text)\n",
    "        word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(word2vec_model.vector_size)\n",
    "        else:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "    return np.vstack(df.apply(average_vector, axis=1))\n",
    "\n",
    "def prepare_and_vectorize_data(df1, df2, word2vec_model, text_extractor):\n",
    "    labels = [0] * len(df1) + [1] * len(df2)\n",
    "    X = pd.concat([df1, df2], ignore_index=True)\n",
    "    X_vectors = vectorize_with_word2vec(X, word2vec_model, text_extractor)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vectors, labels, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_and_evaluate_xgb(X_train, y_train, X_test, y_test, comparison_name):\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"XGBoost Accuracy for {comparison_name}: {accuracy:.6f}\")\n",
    "\n",
    "# Assuming HR_df, HF_df, MR_df, MF_df are DataFrame objects and are loaded correctly\n",
    "word2vec_model = build_word2vec_model([HR_df, HF_df, MR_df, MF_df], lambda row: extract_text(row, 'preprocessed_text', 'text'))\n",
    "\n",
    "# Define pairs for comparison\n",
    "pairs = [\n",
    "    (HR_df, MF_df, \"Human Real vs AI Fake\"),\n",
    "    (HF_df, MF_df, \"Human Fake vs AI Fake\"),\n",
    "    (MR_df, MF_df, \"AI Real vs AI Fake\"),\n",
    "    (HF_df, MR_df, \"Human Fake vs AI Real\"),\n",
    "    (HR_df, MR_df, \"Human Real vs AI Real\"),\n",
    "    (HR_df, HF_df, \"Human Real vs Human Fake\")\n",
    "]\n",
    "\n",
    "for df1, df2, name in pairs:\n",
    "    X_train, X_test, y_train, y_test = prepare_and_vectorize_data(df1, df2, word2vec_model, lambda row: extract_text(row, 'preprocessed_text', 'text'))\n",
    "    train_and_evaluate_xgb(X_train, y_train, X_test, y_test, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf9ad4-f9fe-4943-881a-7c436d12f04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
